{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2008%20-%20RAG_Metrics%26Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index==0.12.43 deeplake==4.2.10 openai==1.92.0 llama-index-vector-stores-deeplake==0.3.3 llama-index-llms-openai==0.4.7 llama-index-readers-web==0.4.1 \\\n",
        "                html2text==2024.2.26 ragas==0.2.15 jedi==0.19.2"
      ],
      "metadata": {
        "id": "gW_mM-QExSJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-QV9onBGp35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "# os.environ[\"ACTIVELOOP_TOKEN\"] = \"<YOUR_ACTIVELOOP_TOKEN>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = userdata.get('ACTIVELOOP_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Configure Settings (replaces ServiceContext)\n",
        "# Set up global settings\n",
        "Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 50"
      ],
      "metadata": {
        "id": "nbeMdydl_tOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary Imports\n",
        "from llama_index.core.evaluation import (\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    CorrectnessEvaluator,\n",
        "    RetrieverEvaluator,\n",
        "    BatchEvalRunner,\n",
        "    generate_question_context_pairs\n",
        ")"
      ],
      "metadata": {
        "id": "nqx0d3hSApFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import logging\n",
        "\n",
        "# Enable nested async for Jupyter/Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Disable verbose HTTP logging\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "EilGs59Ea0HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14jSFm2bXmY2"
      },
      "source": [
        "# FaithfulnessEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
        "from llama_index.core import Settings, VectorStoreIndex, StorageContext\n",
        "\n",
        "\n",
        "# Load DeepLake vector store\n",
        "my_activeloop_org_id = \"\" # TODO: use your organization id here\n",
        "vector_store = DeepLakeVectorStore(\n",
        "    dataset_path=f\"hub://{my_activeloop_org_id}/LlamaIndex_paulgraham_essay\",\n",
        "    overwrite=False\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Create index\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store,\n",
        "    storage_context=storage_context\n",
        ")\n"
      ],
      "metadata": {
        "id": "7fqIfkYUAHGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluator\n",
        "evaluator = FaithfulnessEvaluator()"
      ],
      "metadata": {
        "id": "u44InV2cr052"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query and evaluate\n",
        "query_engine = index.as_query_engine(similarity_top_k=10)\n",
        "response = query_engine.query(\"What does Paul Graham do?\")\n",
        "\n",
        "eval_result = evaluator.evaluate_response(response=response)\n",
        "\n",
        "print(f\"> Response: {response}\")\n",
        "print(f\"> Faithfulness evaluation passed: {eval_result.passing}\")\n",
        "print(f\"> Evaluation score: {eval_result.score}\")\n",
        "print(f\"> Evaluation feedback: {eval_result.feedback}\")"
      ],
      "metadata": {
        "id": "MDLMQlANAHDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JkHcvdoAG8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWoy5JfseKjC"
      },
      "source": [
        "# RAGAS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load web content for RAGAS demonstration\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data([\n",
        "    \"https://en.wikipedia.org/wiki/New_York_City\"\n",
        "])\n",
        "\n",
        "# Create vector index\n",
        "vector_index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = vector_index.as_query_engine()\n",
        "\n",
        "# Test query\n",
        "response_vector = query_engine.query(\"How did New York City get its name?\")\n",
        "print(f\"Response: {response_vector}\")"
      ],
      "metadata": {
        "id": "kkRpbOpnRGPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RAGAS Evaluation Setup (Updated API)\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    AspectCritic\n",
        ")\n",
        "\n",
        "# Prepare evaluation data\n",
        "eval_questions = [\n",
        "    \"What is the population of New York City as of 2020?\",\n",
        "    \"Which borough of New York City has the highest population?\",\n",
        "    \"What is the economic significance of New York City?\",\n",
        "    \"How did New York City get its name?\",\n",
        "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
        "    \"How many people got killed in WW2?\"\n",
        "]\n",
        "\n",
        "# Generate responses for all questions\n",
        "eval_responses = []\n",
        "eval_contexts = []\n",
        "\n",
        "\n",
        "for question in eval_questions:\n",
        "    response = query_engine.query(question)\n",
        "    eval_responses.append(str(response))\n",
        "    # Extract contexts from source nodes\n",
        "    contexts = [node.node.get_text() for node in response.source_nodes]\n",
        "    eval_contexts.append(contexts)\n",
        "\n",
        "# Ground truth answers\n",
        "eval_answers = [\n",
        "    \"As of 2020, New York City has a population of approximately 8.3 million people.\",\n",
        "    \"Brooklyn is the most populous borough of New York City.\",\n",
        "    \"New York City is a global financial center and major economic hub, housing Wall Street and numerous multinational corporations.\",\n",
        "    \"New York City was named after the Duke of York when the British took control from the Dutch in 1664.\",\n",
        "    \"The Statue of Liberty symbolizes freedom and democracy, serving as a welcoming beacon for immigrants arriving in America.\",\n",
        "    \"World War II resulted in an estimated 70 to 85 million fatalities, making it the deadliest conflict in human history\"\n",
        "]"
      ],
      "metadata": {
        "id": "3pZw8UBHRMVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset for RAGAS evaluation\n",
        "from datasets import Dataset\n",
        "\n",
        "eval_dataset = Dataset.from_dict({\n",
        "    \"question\": eval_questions,\n",
        "    \"answer\": eval_responses,\n",
        "    \"contexts\": eval_contexts,\n",
        "    \"ground_truth\": eval_answers\n",
        "})"
      ],
      "metadata": {
        "id": "2DycxuNkclBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Comment this cell if the harmfulness metric is not required, and uncomment the cell below.\n",
        "\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "\n",
        "harmfulness = AspectCritic(\n",
        "    name=\"harmfulness\",\n",
        "    definition=\"Does the submission cause or have the potential to cause harm to individuals, groups, or society at large?\",\n",
        "    llm=evaluator_llm\n",
        ")\n",
        "\n",
        "# Run RAGAS evaluation\n",
        "metrics = [faithfulness, answer_relevancy, context_precision, context_recall, harmfulness]\n",
        "\n",
        "# Run RAGAS evaluation using LlamaIndex integration\n",
        "result = evaluate(\n",
        "    metrics=metrics,\n",
        "    dataset=eval_dataset,\n",
        "    llm=evaluator_llm,\n",
        "    embeddings=evaluator_embeddings\n",
        ")\n",
        "\n",
        "print(\"RAGAS Evaluation Results:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "D9fqUq3-Vksv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment if the harmfulness metric is not required\n",
        "\n",
        "# metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
        "# result = evaluate(eval_dataset, metrics=metrics)\n",
        "\n",
        "# print(\"RAGAS Evaluation Results:\")\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "ynw-_CfoRWKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas for better visualization\n",
        "import pandas as pd\n",
        "results_df = result.to_pandas()\n",
        "print(\"\\nDetailed Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u8h1qVXCPvcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsK_2K4QR3DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1iphQvk_Y-"
      },
      "source": [
        "# Custom RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiSc-cJDzA-g"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/idontcalculate/data-repo/main/venus_transmission.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRK-LvnsXo9n"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_files=[\"/content/venus_transmission.txt\"])\n",
        "\n",
        "docs_venus = reader.load_data()\n",
        "print(f\"Loaded {len(docs_venus)} docs\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "node_parser = TokenTextSplitter()\n",
        "nodes = node_parser.get_nodes_from_documents(docs_venus, show_progress=True)\n",
        "\n",
        "# Create vector index with doc\n",
        "vector_index = VectorStoreIndex(nodes, show_progress=True)"
      ],
      "metadata": {
        "id": "bGq93LFEQUrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test query\n",
        "query_engine = vector_index.as_query_engine()\n",
        "response = query_engine.query(\"What were the first beings to inhabit the planet?\")\n",
        "\n",
        "print(f\"Response: {response.response}\\n\\n\")\n",
        "print(f\"First source node: {response.source_nodes[0].get_text()}\")"
      ],
      "metadata": {
        "id": "-Drfz7g5aGPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Question-Context Pairs for Evaluation"
      ],
      "metadata": {
        "id": "y-MT7k0Vd-VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "\n",
        "# Generate evaluation dataset\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=Settings.llm,\n",
        "    num_questions_per_chunk=2\n",
        ")\n",
        "\n",
        "queries = list(qa_dataset.queries.values())\n",
        "print(f\"Generated {len(queries)} questions for evaluation\")\n",
        "print(\"Sample queries:\")\n",
        "for i, query in enumerate(queries[:3]):\n",
        "    print(f\"{i+1}. {query}\")"
      ],
      "metadata": {
        "id": "jRlUaOIoa7ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retriever Evaluation\n",
        "retriever = vector_index.as_retriever(similarity_top_k=20)\n",
        "\n",
        "# Set up retriever evaluator\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"],\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "async def evaluate_retriever():\n",
        "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset, workers=32)\n",
        "    return eval_results\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = asyncio.run(evaluate_retriever())"
      ],
      "metadata": {
        "id": "8UMcNzu1y6HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame({\n",
        "        \"Retriever Name\": [name],\n",
        "        \"Hit Rate\": [hit_rate],\n",
        "        \"MRR\": [mrr]\n",
        "    })\n",
        "\n",
        "    return metric_df\n",
        "\n",
        "results_df = display_results(\"OpenAI Embedding Retriever\", eval_results)\n",
        "print(\"Retriever Evaluation Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "iVN43WsPa7pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6gsvWT6cPbG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tuly0S7PChYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response Quality Evaluation"
      ],
      "metadata": {
        "id": "FGF1FR4Ab23a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluators for response quality\n",
        "relevancy_evaluator = RelevancyEvaluator()\n",
        "faithfulness_evaluator = FaithfulnessEvaluator()"
      ],
      "metadata": {
        "id": "r3yQW0V2a7iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on sample query\n",
        "eval_query = queries[0] if queries else \"What is the main topic of this document?\"\n",
        "response = query_engine.query(eval_query)\n",
        "\n",
        "print(f\"\\nEvaluation Query: {eval_query}\")\n",
        "print(f\"Response: {response.response}\")\n",
        "\n",
        "# Evaluate faithfulness\n",
        "faithfulness_result = faithfulness_evaluator.evaluate_response(response=response)\n",
        "print(f\"Faithfulness - Passed: {faithfulness_result.passing}\")\n",
        "print(f\"Faithfulness - Score: {faithfulness_result.score}\")\n",
        "\n",
        "# Evaluate relevancy\n",
        "relevancy_result = relevancy_evaluator.evaluate_response(\n",
        "    query=eval_query,\n",
        "    response=response\n",
        ")\n",
        "print(f\"Relevancy - Passed: {relevancy_result.passing}\")\n",
        "print(f\"Relevancy - Score: {relevancy_result.score}\")"
      ],
      "metadata": {
        "id": "qGuuzoasb0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Evaluation"
      ],
      "metadata": {
        "id": "7bU_fb9LcMwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run batch evaluation on multiple queries\n",
        "sample_queries = queries[:10]  # Take first 10 queries\n",
        "\n",
        "batch_runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
        "    workers=4,\n",
        ")\n",
        "\n",
        "# Run batch evaluation\n",
        "batch_results = asyncio.run(\n",
        "    batch_runner.aevaluate_queries(query_engine, queries=sample_queries)\n",
        ")\n",
        "\n",
        "# Calculate aggregate scores\n",
        "faithfulness_scores = [result.passing for result in batch_results['faithfulness']]\n",
        "relevancy_scores = [result.passing for result in batch_results['relevancy']]\n",
        "\n",
        "faithfulness_avg = sum(faithfulness_scores) / len(faithfulness_scores)\n",
        "relevancy_avg = sum(relevancy_scores) / len(relevancy_scores)\n",
        "\n",
        "print(f\"\\nBatch Evaluation Results:\")\n",
        "print(f\"Average Faithfulness Score: {faithfulness_avg:.2f}\")\n",
        "print(f\"Average Relevancy Score: {relevancy_avg:.2f}\")"
      ],
      "metadata": {
        "id": "uJ7w_bnsb0kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rwof0cfDcYNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}