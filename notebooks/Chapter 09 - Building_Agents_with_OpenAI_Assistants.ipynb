{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assistants API\n",
        "\n",
        "https://openai.com/blog/new-models-and-developer-products-announced-at-devday\n",
        "\n",
        "https://platform.openai.com/docs/assistants/overview\n",
        "\n",
        "https://platform.openai.com/playground"
      ],
      "metadata": {
        "id": "DmYE_q3_Abi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U openai sentence-transformers diffusers[\"torch\"] transformers"
      ],
      "metadata": {
        "id": "XpkHBGQ4oNO-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT AND SETUP"
      ],
      "metadata": {
        "id": "rvOMd3AwBwaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "dYd-sK_L-e5r",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from openai import OpenAI\n",
        "# Initialize the client\n",
        "client = OpenAI(\n",
        "  organization=\"towards-ai\",\n",
        ")"
      ],
      "metadata": {
        "id": "r9fxGHSnDGE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "qtEkIVVvCUFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O botsdata.pdf https://raw.githubusercontent.com/jaiganesan-n/dataset_building_ai_for_production/main/botsdata.pdf\n",
        "!wget -O tech_manual.pdf https://raw.githubusercontent.com/jaiganesan-n/dataset_building_ai_for_production/main/tech_manual.pdf"
      ],
      "metadata": {
        "id": "n1Zx-E2BBefJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESPONSES API WITH TECH MANUAL"
      ],
      "metadata": {
        "id": "3xsLTMHMbR-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload Files for Knowledge Base\n",
        "# Upload tech manual for the agent to use\n",
        "file_resp = client.files.create(\n",
        "    file=open(\"tech_manual.pdf\", \"rb\"),\n",
        "    purpose='assistants'  # For Responses API, use 'user_data' instead of 'assistants'\n",
        ")\n",
        "file_id = file_resp.id\n",
        "\n",
        "print(f\"File uploaded: {file_id}\")"
      ],
      "metadata": {
        "id": "v5YbbGIubTuV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector store and upload your file\n",
        "vector_store = client.vector_stores.create(name=\"TechManualStore\")\n",
        "vs_id = vector_store.id\n",
        "client.vector_stores.files.create(vector_store_id=vs_id, file_id=file_resp.id)"
      ],
      "metadata": {
        "id": "M_AH5qiqSDA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a question using file_search\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    instructions=\"You are a robotics expert. Use the manual to answer accurately.\",\n",
        "    input=\"Explain Auditory and Visual (A/V) Warning System\",\n",
        "    tools=[{\n",
        "        \"type\": \"file_search\",\n",
        "        \"vector_store_ids\": [vs_id],\n",
        "        \"max_num_results\": 3\n",
        "    }],\n",
        "    include=[\"output[*].file_search_call.search_results\"]\n",
        ")\n",
        "\n",
        "# Review the assistant’s answer and search metadata\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "yQjTYtEDRz9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_search_call = response.output[0]\n",
        "search_results = file_search_call.results  # ← Correct property\n",
        "\n",
        "for res in search_results:\n",
        "    print(f\"\\n— Source: {res.filename} (score {res.score:.2f})\")\n",
        "    print(\"Excerpt:\", res.text[:200].replace(\"\\n\", \" \"), \"…\")"
      ],
      "metadata": {
        "id": "m0APtG0ISMCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOTSDATA RESEARCH ASSISTANT"
      ],
      "metadata": {
        "id": "4XOgLaokSOF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload botsdata file for the research assistant\n",
        "botsdata_file = client.files.create(\n",
        "    file=open(\"botsdata.pdf\", \"rb\"),\n",
        "    purpose='assistants'\n",
        ")\n",
        "print(f\"Botsdata file uploaded: {botsdata_file.id}\")\n",
        "\n",
        "# Create vector store for botsdata\n",
        "botsdata_vector_store = client.vector_stores.create(name=\"BotsdataStore\")\n",
        "\n",
        "client.vector_stores.files.create(\n",
        "    vector_store_id=botsdata_vector_store.id,\n",
        "    file_id=botsdata_file.id\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    {\"type\": \"file_search\", \"vector_store_ids\": [botsdata_vector_store.id], \"max_num_results\": 3},\n",
        "    {\"type\": \"code_interpreter\", \"container\": {\"type\": \"auto\",\"file_ids\": [botsdata_file.id]}},\n",
        "    {\"type\": \"web_search\"}\n",
        "]"
      ],
      "metadata": {
        "id": "3nmb3Z5TSIwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Research Query 1: Robotics Technology Analysis\n",
        "\n",
        "research_response_1 = client.responses.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "\n",
        "    instructions=\"\"\"You are an advanced research and robotics assistant with expertise in:\n",
        "    - Advanced robotics and automation technologies\n",
        "    - Botnet analysis and cybersecurity (based on your database)\n",
        "    - Technology research and analysis\n",
        "    - Machine learning conference compilation\n",
        "    Use your knowledge base to provide detailed, technical responses.\"\"\",\n",
        "\n",
        "    input=\"Can you provide a detailed analysis of the latest advancements in robotics technology based on the uploaded research data?\",\n",
        "\n",
        "    tools=tools,\n",
        "\n",
        "    include=[\"output[*].file_search_call.search_results\"]\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Robotics Analysis Response:\")\n",
        "print(research_response_1.output_text)"
      ],
      "metadata": {
        "id": "dyl-2QGVcqhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Research Query 2: Conference List Compilation\n",
        "\n",
        "research_response_2 = client.responses.create(\n",
        "     model=\"gpt-4.1-mini\",\n",
        "\n",
        "    instructions=\"\"\"You are an expert research assistant. Compile comprehensive lists of academic conferences,\n",
        "    especially in machine learning, robotics, and cybersecurity. Use both your knowledge base and web search.\"\"\",\n",
        "\n",
        "    input=\"Compile a list of upcoming machine learning conferences including dates, locations, and focus areas for 2025\",\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "print(\"\\nML Conferences List:\")\n",
        "print(research_response_2.output_text)"
      ],
      "metadata": {
        "id": "y8M6kvNrcqex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Research Query 3: Botnet Analysis (based on database content)\n",
        "botnet_response = client.responses.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "\n",
        "    instructions=\"\"\"Based on your database about botnets, provide expert analysis on botnet detection and mitigation.\n",
        "    Your database includes information on command-and-control structures, bot families, and detection methods.\"\"\",\n",
        "\n",
        "    input=\"Analyze the behavior patterns of different botnet families and recommend detection strategies\",\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "print(\"\\nBotnet Analysis:\")\n",
        "print(botnet_response.output_text)"
      ],
      "metadata": {
        "id": "BRyFI0BcSIqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HUGGINGFACE API &InferenceApi INTEGRATIONS"
      ],
      "metadata": {
        "id": "vPgYQSNmbLKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Sentiment Analysis Workflow\n",
        "headers = {\"Authorization\": f\"Bearer {userdata.get('HF_TOKEN')}\"}\n",
        "\n",
        "# Sentiment analysis with updated model\n",
        "sentiment_url = \"https://api-inference.huggingface.co/models/siebert/sentiment-roberta-large-english\"\n",
        "sentiment_inputs = {\n",
        "    \"inputs\": [\"I love this product!\", \"I'm frustrated by the frequent errors in the software's latest update\"]\n",
        "}\n",
        "\n",
        "sentiment_response = requests.post(sentiment_url, headers=headers, json=sentiment_inputs)\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(sentiment_response.json())"
      ],
      "metadata": {
        "id": "R9O6fexXbIVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(provider=\"hf-inference\")"
      ],
      "metadata": {
        "id": "qvZHvXVpq6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Classification\n",
        "response = client.text_classification(\n",
        "    text=\"I love how this app simplifies complex tasks effortlessly. I'm frustrated by the frequent errors in the software's latest update\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "spgtqnCRqAZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import InferenceClient\n",
        "\n",
        "# client = InferenceClient(provider=\"hf-inference\")\n",
        "\n",
        "# response = client.text_generation(\n",
        "#     prompt=\"The new president of America will be\",\n",
        "#     model=\"gpt2\",\n",
        "# )\n",
        "\n",
        "# print(response.generated_text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YkBc5MsGy385"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Generation Workflow"
      ],
      "metadata": {
        "id": "lfwe6tTR6cI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "import torch\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Load current Stable Diffusion model with optimizations\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False\n",
        ")\n",
        "\n",
        "# Use faster scheduler\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# Use faster scheduler\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# Choose memory optimization strategy based on available resources\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if device == \"cuda\":\n",
        "    # GPU available - use direct GPU placement for best performance\n",
        "    pipe = pipe.to(device)\n",
        "    # Enable attention slicing for memory efficiency\n",
        "    if hasattr(pipe, 'enable_attention_slicing'):\n",
        "        pipe.enable_attention_slicing()\n",
        "else:\n",
        "    # CPU only - use model offloading for memory efficiency\n",
        "    if hasattr(pipe, 'enable_model_cpu_offload'):\n",
        "        pipe.enable_model_cpu_offload()\n",
        "    if hasattr(pipe, 'enable_attention_slicing'):\n",
        "        pipe.enable_attention_slicing()\n",
        "\n",
        "# Generate futuristic cityscape\n",
        "prompt = \"Create an image of a futuristic cityscape on an alien planet, featuring towering skyscrapers with glowing neon lights, a sky filled with multiple moons, and inhabitants of various alien species walking through vibrant market streets\"\n",
        "\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    num_inference_steps=25,\n",
        "    guidance_scale=7.5,\n",
        "    height=512,\n",
        "    width=512\n",
        ").images[0]\n",
        "\n",
        "image.save(\"futuristic_cityscape.png\")\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "gRAU6mjzy345"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENTENCE TRANSFORMERS EMBEDDINGS"
      ],
      "metadata": {
        "id": "v3YwemRz87cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentences = [\n",
        "    \"GAIA's questions are rooted in practical use cases, requiring AI systems to interact with a diverse and uncertain world, reflecting real-world applications.\",\n",
        "    \"GAIA questions require accurate execution of complex sequences of actions, akin to the Proof of Work concept, where the solution is simple to verify but challenging to generate.\"\n",
        "]\n",
        "\n",
        "# Use current high-performance model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Sample embedding dimensions: {embeddings[0][:10]}\")"
      ],
      "metadata": {
        "id": "g3XyKSZOy3ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMAGE PROCESSING AND ANALYSIS"
      ],
      "metadata": {
        "id": "0YW2W5RC9Q4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, ViTImageProcessor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "model_name=\"nlpconnect/vit-gpt2-image-captioning\"\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Image Captioning\n",
        "image_captioner = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=model_name,\n",
        "    feature_extractor=processor,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Caption sample image\n",
        "caption_result = image_captioner(\"https://ankur3107.github.io/assets/images/image-captioning-example.png\")\n",
        "print(\"Image Captioning Result:\", caption_result)"
      ],
      "metadata": {
        "id": "t5Exop5jy3pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Classification with ViT\n",
        "image_url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "outputs = vit_model(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", vit_model.config.id2label[predicted_class_idx])"
      ],
      "metadata": {
        "id": "PFSOcS-V_Yl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEB CONTENT EXTRACTION AND PROCESSING"
      ],
      "metadata": {
        "id": "--FsN0Tr9g3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Web content extraction\n",
        "def extract_web_content(url):\n",
        "    \"\"\"Extract content from URL using requests (fallback method)\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        return response.text[:5000]\n",
        "    except:\n",
        "        return \"Could not extract content from URL\"\n",
        "\n",
        "# Example content extraction\n",
        "sample_url = \"https://techcrunch.com/2023/11/25/neuralink-elon-musks-brain-implant-startup-quietly-raises-an-additional-43m/\"\n",
        "extracted_content = extract_web_content(sample_url)\n",
        "print(f\"Extracted content length: {len(extracted_content)} characters\")"
      ],
      "metadata": {
        "id": "n-jS47k8bICQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_content(text):\n",
        "    \"\"\"Summarize text using HuggingFace API\"\"\"\n",
        "    if not text.strip():\n",
        "        return {\"error\": \"No text to summarize\"}\n",
        "\n",
        "    summarizer_url = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
        "\n",
        "    # Truncate text if too long\n",
        "    max_length = 2000\n",
        "    if len(text) > max_length:\n",
        "        text = text[:max_length]\n",
        "\n",
        "    response = requests.post(\n",
        "        summarizer_url,\n",
        "        headers={\"Authorization\": f\"Bearer {userdata.get('HF_TOKEN')}\"},\n",
        "        json={\n",
        "            \"inputs\": text,\n",
        "            \"parameters\": {\n",
        "                \"max_length\": 250,\n",
        "                \"min_length\": 50,\n",
        "                \"do_sample\": False\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "# Summarize extracted content\n",
        "summary_result = summarize_content(extracted_content)\n",
        "print(\"Summarization Result:\")\n",
        "print(summary_result)"
      ],
      "metadata": {
        "id": "frHCzUACbH8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dz-ehpE0OP_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}