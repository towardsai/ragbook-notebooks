{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Autonomous Agents to Create Analysis Reports"
      ],
      "metadata": {
        "id": "v-FzkIqP_u6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q deeplake==4.2.11 langchain==0.3.26 langchain-openai==0.3.23 langchain-core==0.3.66 langchain-deeplake==0.1.0 langchain-community==0.3.26 \\\n",
        "                openai==1.92.0 jedi==0.19.2 lxml_html_clean==0.4.2 newspaper4k==0.9.3 langgraph==0.5.0 langchain-text-splitters==0.3.8"
      ],
      "metadata": {
        "id": "df_n4ozAa0ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
        "# os.environ[\"ACTIVELOOP_TOKEN\"] = \"<YOUR-ACTIVELOOP-TOKEN>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = userdata.get('ACTIVELOOP_TOKEN')"
      ],
      "metadata": {
        "id": "yAmImWVk_t_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape articles\n",
        "import requests\n",
        "from newspaper import Article\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Fixed: Added missing comma in the URL list\n",
        "article_urls = [\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/23/meta-open-source-speech-ai-models-support-over-1100-languages/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/18/beijing-launches-campaign-against-ai-generated-misinformation/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/16/openai-ceo-ai-regulation-is-essential/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/15/jay-migliaccio-ibm-watson-on-leveraging-ai-to-improve-productivity/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/15/iurii-milovanov-softserve-how-ai-ml-is-helping-boost-innovation-and-personalisation/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/11/ai-and-big-data-expo-north-america-begins-in-less-than-one-week/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/11/eu-committees-green-light-ai-act/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/09/wozniak-warns-ai-will-power-next-gen-scams/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/09/infocepts-ceo-shashank-garg-on-the-da-market-shifts-and-impact-of-ai-on-data-analytics/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/02/ai-godfather-warns-dangers-and-quits-google/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/28/palantir-demos-how-ai-can-used-military/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/26/ftc-chairwoman-no-ai-exemption-to-existing-laws/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/24/bill-gates-ai-teaching-kids-literacy-within-18-months/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/21/google-creates-new-ai-division-to-challenge-openai/\"\n",
        "]\n",
        "\n",
        "session = requests.Session()\n",
        "pages_content = []\n",
        "\n",
        "for url in article_urls:\n",
        "    try:\n",
        "        time.sleep(2)\n",
        "        response = session.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            pages_content.append({\"url\": url, \"text\": article.text})\n",
        "        else:\n",
        "            print(f\"Failed to fetch article at {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while fetching article at {url}: {e}\")\n",
        "\n",
        "print(f\"Successfully scraped {len(pages_content)} articles\")"
      ],
      "metadata": {
        "id": "72DqzOyC1Yt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up embeddings and DeepLake vector store\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_deeplake.vectorstores import DeeplakeVectorStore\n",
        "\n",
        "# Use updated OpenAI embeddings\n",
        "embeddings = OpenAIEmbeddings( model=\"text-embedding-3-small\")\n",
        "\n",
        "# Configure DeepLake dataset path\n",
        "\n",
        "# Option 1: Local storage (faster for development)\n",
        "dataset_path_local = \"./my_deeplake_analysis/\"\n",
        "\n",
        "# Option 2: Activeloop cloud storage\n",
        "#\n",
        "activeloop_org_id = \"\"  # TODO: Update this with your org id\n",
        "dataset_path_cloud = f\"hub://{activeloop_org_id}/langchain_ai_analysis\"\n",
        "\n",
        "# Choose your preferred storage option\n",
        "USE_CLOUD_STORAGE = True  # Set to True to use Activeloop cloud storage\n",
        "\n",
        "if USE_CLOUD_STORAGE:\n",
        "    dataset_path = dataset_path_cloud\n",
        "    print(f\"Using Activeloop cloud storage: {dataset_path}\")\n",
        "else:\n",
        "    dataset_path = dataset_path_local\n",
        "    print(f\"Using local storage: {dataset_path}\")\n",
        "\n",
        "print(f\"DeepLake dataset will be stored at: {dataset_path}\")"
      ],
      "metadata": {
        "id": "kJ-UL1JQ1YqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents and create DeepLake vector store\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "# Process documents and create Document objects with metadata\n",
        "documents = []\n",
        "for i, content in enumerate(pages_content):\n",
        "    chunks = text_splitter.split_text(content[\"text\"])\n",
        "    for chunk in chunks:\n",
        "        documents.append(Document(\n",
        "            page_content=chunk,\n",
        "            metadata={\n",
        "                \"source\": content[\"url\"],\n",
        "                \"article_id\": i,\n",
        "                \"chunk_type\": \"article_content\"\n",
        "            }\n",
        "        ))\n",
        "\n",
        "print(f\"Created {len(documents)} document chunks\")"
      ],
      "metadata": {
        "id": "pzcYEnTV1Ynx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DeepLake vector store and add documents\n",
        "try:\n",
        "    # Create the vector store with documents\n",
        "    vectorstore = DeeplakeVectorStore.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        dataset_path=dataset_path,\n",
        "        overwrite=True  # Set to False if you want to append to existing data\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully created DeepLake vector store with {len(documents)} documents!\")\n",
        "    print(f\"Dataset path: {dataset_path}\")\n",
        "\n",
        "    # Display some basic information about the dataset\n",
        "    if hasattr(vectorstore, 'dataset'):\n",
        "        print(f\"Dataset info: {vectorstore.dataset.info}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating DeepLake vector store: {e}\")\n",
        "\n",
        "    # Fallback to local storage\n",
        "    print(\"\\nFalling back to local storage...\")\n",
        "    dataset_path = dataset_path_local\n",
        "    vectorstore = DeeplakeVectorStore.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        dataset_path=dataset_path,\n",
        "        overwrite=True\n",
        "    )\n",
        "    print(f\"Successfully created local DeepLake vector store!\")"
      ],
      "metadata": {
        "id": "-8DKIeQF1YlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create retrieval tool with DeepLake\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# Get retriever from vector store\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "\n",
        "def retrieve_ai_news(query: str) -> str:\n",
        "    \"\"\"Searches for relevant AI news documents that may contain the answer to the query.\"\"\"\n",
        "    try:\n",
        "        docs = retriever.invoke(query)\n",
        "\n",
        "        if not docs:\n",
        "            return \"No relevant documents found.\"\n",
        "\n",
        "        # Format the results\n",
        "        results = []\n",
        "        for i, doc in enumerate(docs, 1):\n",
        "            content = doc.page_content\n",
        "            source = doc.metadata.get('source', 'Unknown source')\n",
        "            article_id = doc.metadata.get('article_id', 'Unknown')\n",
        "            results.append(f\"--- Document {i} (Article {article_id}) ---\\nSource: {source}\\nContent: {content}\\n\")\n",
        "\n",
        "        return \"\\n\".join(results)\n",
        "    except Exception as e:\n",
        "        return f\"Error retrieving documents: {str(e)}\"\n",
        "\n",
        "# Create the tool\n",
        "search_tool = Tool(\n",
        "    name=\"Search_AI_News\",\n",
        "    func=retrieve_ai_news,\n",
        "    description=\"Search for relevant AI news and information to answer questions about artificial intelligence developments, regulations, and industry trends.\"\n",
        ")\n",
        "\n",
        "print(\"Retrieval tool created successfully!\")"
      ],
      "metadata": {
        "id": "zp8Wsb351Yiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent using LangGraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "\n",
        "# Create memory saver for conversation history\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Create the agent using LangGraph's create_react_agent\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[search_tool],\n",
        "    checkpointer=memory,\n",
        "    prompt=\"\"\"You are an AI research analyst specializing in artificial intelligence news and trends.\n",
        "    You have access to a comprehensive DeepLake vector database containing recent AI news articles.\n",
        "\n",
        "    Use the Search_AI_News tool to find relevant information before answering questions.\n",
        "\n",
        "    When creating reports or analyses:\n",
        "    1. Search for relevant information using the available tool\n",
        "    2. Analyze the information critically\n",
        "    3. Provide well-structured, comprehensive responses\n",
        "    4. Include specific examples and data when available\n",
        "    5. Cite sources when mentioning specific information\n",
        "    6. Consider multiple perspectives when discussing controversial topics\n",
        "\n",
        "    Always search for information before providing answers, even if you think you know the answer.\n",
        "\n",
        "    The DeepLake vector store contains articles about AI developments, regulations, industry trends,\n",
        "    and expert opinions from 2023. Use this rich dataset to provide informed, data-driven responses.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"Agent created successfully!\")"
      ],
      "metadata": {
        "id": "ygpOuQoo1Yf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the agent\n",
        "config = {\"configurable\": {\"thread_id\": \"analysis_session\"}}\n",
        "\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write an overview of Artificial Intelligence regulations by governments by country\"}]},\n",
        "    config\n",
        ")\n",
        "\n",
        "\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "0GBOcQdj1Ydh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDSDMTyf1Ya5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}