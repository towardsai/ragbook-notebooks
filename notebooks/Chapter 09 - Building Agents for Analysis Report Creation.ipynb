{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Autonomous Agents to Create Analysis Reports"
      ],
      "metadata": {
        "id": "v-FzkIqP_u6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = \"<YOUR-ACTIVELOOP-TOKEN>\""
      ],
      "metadata": {
        "id": "yAmImWVk_t_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We scrape several Artificial Intelligence news\n",
        "\n",
        "import requests\n",
        "from newspaper import Article # https://github.com/codelucas/newspaper\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
        "}\n",
        "\n",
        "article_urls = [\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/23/meta-open-source-speech-ai-models-support-over-1100-languages/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/18/beijing-launches-campaign-against-ai-generated-misinformation/\"\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/16/openai-ceo-ai-regulation-is-essential/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/15/jay-migliaccio-ibm-watson-on-leveraging-ai-to-improve-productivity/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/15/iurii-milovanov-softserve-how-ai-ml-is-helping-boost-innovation-and-personalisation/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/11/ai-and-big-data-expo-north-america-begins-in-less-than-one-week/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/11/eu-committees-green-light-ai-act/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/09/wozniak-warns-ai-will-power-next-gen-scams/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/09/infocepts-ceo-shashank-garg-on-the-da-market-shifts-and-impact-of-ai-on-data-analytics/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/05/02/ai-godfather-warns-dangers-and-quits-google/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/28/palantir-demos-how-ai-can-used-military/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/26/ftc-chairwoman-no-ai-exemption-to-existing-laws/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/24/bill-gates-ai-teaching-kids-literacy-within-18-months/\",\n",
        "    \"https://www.artificialintelligence-news.com/2023/04/21/google-creates-new-ai-division-to-challenge-openai/\"\n",
        "]\n",
        "\n",
        "session = requests.Session()\n",
        "pages_content = [] # where we save the scraped articles\n",
        "\n",
        "for url in article_urls:\n",
        "    try:\n",
        "        time.sleep(2) # sleep two seconds for gentle scraping\n",
        "        response = session.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            article = Article(url)\n",
        "            article.download() # download HTML of webpage\n",
        "            article.parse() # parse HTML to extract the article text\n",
        "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
        "        else:\n",
        "            print(f\"Failed to fetch article at {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while fetching article at {url}: {e}\")\n",
        "\n",
        "#If an error occurs while fetching an article, we catch the exception and print\n",
        "#an error message. This ensures that even if one article fails to download,\n",
        "#the rest of the articles can still be processed."
      ],
      "metadata": {
        "id": "1GEQJGYI_uOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use an embedding model to compute our documents' embeddings\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# We'll store the documents and their embeddings in the deep lake vector db\n",
        "from langchain.vectorstores import DeepLake\n",
        "\n",
        "# Setup deep lake\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
        "my_activeloop_dataset_name = \"langchain_course_analysis_outline\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "TodXWoGl_uWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We split the article texts into small chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "all_texts = []\n",
        "for d in pages_content:\n",
        "    chunks = text_splitter.split_text(d[\"text\"])\n",
        "    for chunk in chunks:\n",
        "        all_texts.append(chunk)"
      ],
      "metadata": {
        "id": "eqQJhYeO_uY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we add all the chunks to the Deep lake\n",
        "db.add_texts(all_texts)"
      ],
      "metadata": {
        "id": "1YIGjRKU_7ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the retriever object from the deep lake db object and set the number\n",
        "# of retrieved documents to 3\n",
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['k'] = 3\n",
        "\n",
        "# We define some variables that will be used inside our custom tool\n",
        "CUSTOM_TOOL_DOCS_SEPARATOR =\"\\n---------------\\n\" # how to join together the retrieved docs to form a single string\n",
        "\n",
        "# This is the function that defines our custom tool that retrieves relevant\n",
        "# docs from Deep Lake\n",
        "def retrieve_n_docs_tool(query: str) -> str:\n",
        "    \"\"\"Searches for relevant documents that may contain the answer to the query.\"\"\"\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    texts = [doc.page_content for doc in docs]\n",
        "    texts_merged = \"---------------\\n\" + CUSTOM_TOOL_DOCS_SEPARATOR.join(texts) + \"\\n---------------\"\n",
        "    return texts_merged"
      ],
      "metadata": {
        "id": "bXPGNQ1q_8vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.tools import Tool\n",
        "\n",
        "# We create the tool that uses the \"retrieve_n_docs_tool\" function\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search Private Docs\",\n",
        "        func=retrieve_n_docs_tool,\n",
        "        description=\"useful for when you need to answer questions about current events about Artificial Intelligence\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "36ZfLG8O_7dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
        "\n",
        "# let's create the Plan and Execute agent\n",
        "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "planner = load_chat_planner(model)\n",
        "executor = load_agent_executor(model, tools, verbose=True)\n",
        "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)"
      ],
      "metadata": {
        "id": "8_TNPpwo_7fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we test the agent\n",
        "response = agent.run(\"Write an overview of Artificial Intelligence regulations by governments by country\")"
      ],
      "metadata": {
        "id": "TjVjCYZJ_7iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "QBZfIjTW_7px"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}