{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Chains_and_Why_They_Are_Used.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZgUJsmpUCUi",
        "outputId": "7c27b0d4-5fad-4618-e063-68e87de1e81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.7/823.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucL9y4VoUJui"
      },
      "outputs": [],
      "source": [
        "!echo \"OPENAI_API_KEY='<API_KEY>'\" > .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAsSxpAyUOnF",
        "outputId": "89d0a63f-9c88-4a8f-958a-e97c7451a9f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-J9LsngZsfp"
      },
      "source": [
        "# Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44NeYe0GXe32"
      },
      "source": [
        "## __ call __"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQn4558HUPvI",
        "outputId": "35edbf54-a44e-41d7-806f-2d2e0f374970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "\n",
        "prompt_template = \"What is a word to replace the following: {word}?\"\n",
        "\n",
        "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate.from_template(prompt_template)\n",
        ")\n",
        "llm_chain(\"artificial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lGTqvJxXjMZ"
      },
      "source": [
        "## Apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhrI8CggVtJo",
        "outputId": "f2673059-65b2-4ba4-df2c-063688277c88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_list = [\n",
        "    {\"word\": \"artificial\"},\n",
        "    {\"word\": \"intelligence\"},\n",
        "    {\"word\": \"robot\"}\n",
        "]\n",
        "\n",
        "llm_chain.apply(input_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5jB5LBJXk9s"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYi0o5KqV68n",
        "outputId": "e656827f-2299-423e-f4a8-af09a1402fae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 33, 'completion_tokens': 13, 'total_tokens': 46}, 'model_name': 'text-davinci-003'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.generate(input_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4ej9ZXXm-E"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q1BvtRiXuLg"
      },
      "source": [
        "#### Multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OX4RrRrlXvWm",
        "outputId": "c9cc9d8a-1585-4f6d-ca5f-e752c04c1fd2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template = \"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
        "\n",
        "llm_chain.predict(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NpdjaLWwYQQ1",
        "outputId": "5ebe6455-95a2-4167-e235-d49982f9fe0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nAdmirer'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict(word=\"fan\", context=\"humans\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9P5gGwxClJeL",
        "outputId": "560ea2b2-b540-42a8-c18a-19fcca6f0e70"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.run(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNPOT6iAbt1l"
      },
      "source": [
        "### from string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6T5_9k2bx_N"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\"\"\n",
        "llm_chain = LLMChain.from_string(llm=llm, template=template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AkE6wx8Vb9Ns",
        "outputId": "dbedb888-49d7-43da-88a5-05472fbea85d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRIaIXSKZu6U"
      },
      "source": [
        "# Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aIEZWDQtZwKw",
        "outputId": "aa45ef77-7ea6-42ea-c613-94ac0accfb88"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nSynthetic, Manufactured, Imitation, Fabricated, Fake, Simulated, Artificial Intelligence, Automated, Constructed, Programmed, Mechanical, Processed, Algorithmic, Generated.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[], output_parser=output_parser))\n",
        "\n",
        "llm_chain.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ18G38-aXcE",
        "outputId": "248f1533-6948-4f94-be8d-1ceb88433c20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Synthetic',\n",
              " 'Manufactured',\n",
              " 'Imitation',\n",
              " 'Fabricated',\n",
              " 'Fake',\n",
              " 'Simulated',\n",
              " 'Artificial Intelligence',\n",
              " 'Automated',\n",
              " 'Constructed',\n",
              " 'Programmed',\n",
              " 'Processed',\n",
              " 'Mechanical',\n",
              " 'Man-Made',\n",
              " 'Lab-Created',\n",
              " 'Artificial Neural Network.']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict_and_parse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b8_oGlAcx3F"
      },
      "source": [
        "# Conversational Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTYpcUktae6w",
        "outputId": "3c7c83dd-5ba6-44f6-bbe7-baf153c97232"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Synthetic', 'Manufactured', 'Imitation']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "\n",
        "{input}\"\"\"\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
        "    memory=ConversationBufferMemory())\n",
        "\n",
        "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7_vgaBgeWWG",
        "outputId": "0e9c50d3-a48f-4083-bafe-4944a53dda26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Fabricated', 'Simulated', 'Automated', 'Constructed']"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict_and_parse(input=\"And the next 4?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kz12V6FhjC3"
      },
      "source": [
        "# Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGRoPJ1xhtTE",
        "outputId": "4bfd9840-47a9-4967-8c9d-e6a9ff0fbaa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "\n",
            "Answer briefly. write the first 3 options.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Synthetic', 'Manufactured', 'Imitation']"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
        "    memory=ConversationBufferMemory(),\n",
        "    verbose=True)\n",
        "\n",
        "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XI9e40ui1yX"
      },
      "source": [
        "# Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A16wajt2hxLE"
      },
      "outputs": [],
      "source": [
        "# from langchain.chains import SimpleSequentialChain\n",
        "# overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs4Chc0iKaj3"
      },
      "source": [
        "# Custom Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tCjI4DtKbTG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "class ConcatenateChain(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        # Union of the input keys of the two chains.\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return ['concat_output']\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
        "        output_1 = self.chain_1.run(inputs)\n",
        "        output_2 = self.chain_2.run(inputs)\n",
        "        return {'concat_output': output_1 + output_2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-W3ZqALLbwP",
        "outputId": "83f65c01-5573-403f-9180-7a2b60a41b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated output:\n",
            "\n",
            "\n",
            "Artificial means something that is not natural or made by humans, but rather created or produced by artificial means.\n",
            "\n",
            "Synthetic\n"
          ]
        }
      ],
      "source": [
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"word\"],\n",
        "    template=\"What is the meaning of the following word '{word}'?\",\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"word\"],\n",
        "    template=\"What is a word to replace the following: {word}?\",\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"artificial\")\n",
        "print(f\"Concatenated output:\\n{concat_output}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPESOSrhtfDiEeFVbO8r7kg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
